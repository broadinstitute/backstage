# kubernetes/backstage.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backstage
  namespace: backstage
  labels:
    app: backstage
    environment: prod
    team: devnull
    app.kubernetes.io/managed-by: gcp-cloud-build-deploy
    backstage.io/kubernetes-id: backstage
spec:
  selector:
    matchLabels:
      app: backstage
      environment: prod
      app.kubernetes.io/managed-by: gcp-cloud-build-deploy
      backstage.io/kubernetes-id: backstage
  template:
    metadata:
      labels:
        app: backstage
        environment: prod
        app.kubernetes.io/managed-by: gcp-cloud-build-deploy
        backstage.io/kubernetes-id: backstage
        team: devnull
    spec:
      serviceAccountName: backstage
      containers:
        - name: backstage
          image: us-east4-docker.pkg.dev/bits-gke-clusters/bits/backstage:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: NODE_ENV
              value: production # In this context production means "running in GKE"
            - name: TECHDOCS_STORAGE_BUCKET
              value: bits-backstage-prod-techdocs # kpt-set: ${project-id}-techdocs
            - name: POSTGRES_HOST
              value: localhost
            - name: POSTGRES_PORT
              value: "5432"
            - name: POSTGRES_USER
              value: backstage@bits-backstage-prod.iam # kpt-set: ${db_user}
            - name: BACKSTAGE_APPLICATION_BASE_URL
              value: https://backstage.broadinstitute.org # kpt-set: ${backstage_application_base_url}
            - name: PD_ACCOUNT_SUBDOMAIN
              value: broadinstitute
            - name: BRANCH_NAME
              value: main # kpt-set: ${branch_name}
            - name: OTEL_SERVICE_NAME
              value: backstage
          resources:
            limits:
              cpu: "1"
              memory: "2G"
            requests:
              cpu: "1"
              memory: "2G"
          volumeMounts:
            - name: backstage-secrets
              mountPath: /workspace/secrets
          ports:
            - name: http
              containerPort: 7007
              # Uncomment if health checks are enabled in your app:
              # https://backstage.io/docs/plugins/observability#health-checks
          readinessProbe:
            httpGet:
              port: 7007
              path: /.backstage/health/v1/readiness
            initialDelaySeconds: 60
              #          livenessProbe:
              #            httpGet:
              #              port: 7007
              #              path: /healthcheck
      initContainers:
        - name: cloud-sql-proxy
          # It is recommended to use the latest version of the Cloud SQL Auth Proxy
          # Make sure to update on a regular schedule!
          image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.17.1
          restartPolicy: Always # Run as a sidecar container
          args:
            # If connecting from a VPC-native GKE cluster, you can use the
            # following flag to have the proxy connect over private IP
            # - "--private-ip"

            # Enable structured logging with LogEntry format:
            - --structured-logs
            # Replace DB_PORT with the port the proxy should listen on
            - --port=5432
            # Use auto iam authn to authenticate with the Cloud SQL instance
            - --auto-iam-authn
            - --health-check
            - --debug-logs
            - bits-backstage-prod:us-east4:backstage-prod # kpt-set: ${instance_connection_name}
          securityContext:
            # The default Cloud SQL Auth Proxy image runs as the
            # "nonroot" user and group (uid: 65532) by default.
            runAsNonRoot: true
          # You should use resource requests/limits as a best practice to prevent
          # pods from consuming too many resources and affecting the execution of
          # other pods. You should adjust the following values based on what your
          # application needs. For details, see
          # https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
          resources:
            requests:
              # The proxy's memory use scales linearly with the number of active
              # connections. Fewer open connections will use less memory. Adjust
              # this value based on your application's requirements.
              memory: "2G"
              # The proxy's CPU use scales linearly with the amount of IO between
              # the database and the application. Adjust this value based on your
              # application's requirements.
              cpu: "1"
            limits:
              memory: "2G"
              cpu: "1"
            # Recommended configurations for health check probes.
            # Probe parameters can be adjusted to best fit the requirements of your application.
            # For details, see https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
          # startupProbe:
          #   # We recommend adding a startup probe to the proxy sidecar
          #   # container. This will ensure that service traffic will be routed to
          #   # the pod only after the proxy has successfully started.
          #   httpGet:
          #     path: /startup
          #     port: 9090
          #   periodSeconds: 1
          #   timeoutSeconds: 5
          #   failureThreshold: 20
          # livenessProbe:
          #   # We recommend adding a liveness probe to the proxy sidecar container.
          #   httpGet:
          #     path: /liveness
          #     port: 9801
          #   # Number of seconds after the container has started before the first probe is scheduled. Defaults to 0.
          #   # Not necessary when the startup probe is in use.
          #   initialDelaySeconds: 0
          #   # Frequency of the probe.
          #   periodSeconds: 60
          #   # Number of seconds after which the probe times out.
          #   timeoutSeconds: 30
          #   # Number of times the probe is allowed to fail before the transition
          #   # from healthy to failure state.
          #   #
          #   # If periodSeconds = 60, 5 tries will result in five minutes of
          #   # checks. The proxy starts to refresh a certificate five minutes
          #   # before its expiration. If those five minutes lapse without a
          #   # successful refresh, the liveness probe will fail and the pod will be
          #   # restarted.
          #   failureThreshold: 5
          # We do not recommend adding a readiness probe under most circumstances

      volumes:
        - name: backstage-secrets
          csi:
            driver: secrets-store-gke.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: backstage
